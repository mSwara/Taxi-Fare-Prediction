# -*- coding: utf-8 -*-
"""Taxi_Fare_Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TW7xZjtGERCAlTSGaOG4jDQHifasLr7W

#Taxi Fare Predictor

**Description -**
- As target variable is already known, we treat this as a Supervised Learning problem.
- Fare is a continuous real variable, so regression models will be used.
- As Taxi Fare is extremely dependent on Fuel price, we have used a public dataset which includes weekly average of fuel prices in New York City

**Steps -**
1. Downloading dataset, Installing packages & Loading dataset
2. Exploratory Data Analysis & Visualization
3. Preparing the dataset for Machine Learning
4. Training Hardcoded & Baseline Models
5. Feature Engineering
6. Training & Evaluating Different Models
7. Predicting Fares for test data & Creating the submission file

##1. Downloading dataset, Installing packages & Loading dataset

###Installing packages & downloading dataset
"""

!pip install opendatasets --quiet

import opendatasets as od

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

download_url = 'https://www.kaggle.com/competitions/new-york-city-taxi-fare-prediction/'
download_url_fuel = 'https://www.kaggle.com/datasets/new-york-state/nys-diesel-retail-price-weekly-average'

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c new-york-city-taxi-fare-prediction

"""Upload the kaggle API key json file from kaggle or input the username & API key when asked after running the following two cells"""

od.download(download_url)

od.download(download_url_fuel)

"""###Viewing Dataset Files"""

data_dir = 'new-york-city-taxi-fare-prediction/'

data_dir_fuel = 'nys-diesel-retail-price-weekly-average/'

!ls -lh {data_dir} #'!' means run the command in shell

!wc -l {data_dir}train.csv

!wc -l {data_dir}test.csv

!wc -l {data_dir}sample_submission.csv

!head {data_dir}train.csv

!head {data_dir}test.csv

"""###Loading the dataset"""

import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt

cols = ('fare_amount,pickup_datetime,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude,passenger_count').split(',')
print(cols)

dtypes = {
    'fare_amount':'float32',
    'pickup_longitude':'float32',
    'pickup_latitude':'float32',
    'dropoff_longitude':'float32',
    'dropoff_latitude':'float32',
    'passenger_count':'uint8' #count can't be nagative, so we use unsigned int datatype
}

sample_fraction = 0.03 #We are going to use only 3% dataset
random.seed(42) #Fixing the seed to get same results everytime

#Utility Function to randomly read only 3% rows from the csv file
def skip_row(row_idx):
  if row_idx == 0:
    return False
  else:
    return random.random() > sample_fraction

"""Loading the training data"""

df = pd.read_csv(
    data_dir+'train.csv',
    usecols=cols,
    parse_dates=['pickup_datetime'],
    dtype=dtypes,
    skiprows=skip_row)

test_df = pd.read_csv(
    data_dir+'/test.csv',
    dtype=dtypes,
    parse_dates=['pickup_datetime'])

df_fuel = pd.read_csv(
    data_dir_fuel+'diesel-retail-price-weekly-average-by-region-beginning-2007.csv',
    parse_dates=['Date'],
    usecols=['Date', 'New York State Average ($/gal)'])

"""##2. Exploratory Data Analysis & Visualization

####Training Set
"""

df.head()

df.info()

df.describe()

"""Time Period of Records"""

df.pickup_datetime.min(), df.pickup_datetime.max()

"""Distribution of number of records in each year"""

df.pickup_datetime.dt.year.value_counts().plot(kind='barh')

"""Number of rows from each year are quite uniform, except for year 2015 as the dataset was recorded till June, 2015

Percentage of weekend trips
"""

len(df[df.pickup_datetime.dt.dayofweek.isin([5,6])])/len(df)*100

"""####Test Data"""

test_df.head()

test_df.info()

test_df.describe()

"""Time period of test records"""

test_df.pickup_datetime.min(),test_df.pickup_datetime.max()

"""28% records are of weekends

####Fuel Data
"""

df_fuel.head()

len(df_fuel)

"""Time period of fuel data records"""

df_fuel.Date.dt.year.min(), df_fuel.Date.dt.year.max()

plt.plot(df_fuel.Date.dt.date.values,df_fuel['New York State Average ($/gal)'].values)
plt.show()

"""##3. Preparing the dataset for Machine Learning

###Splitting Training & Validation Set
"""

X = df.iloc[:,1:].values
y = df.iloc[:,1].values

from sklearn.model_selection import train_test_split
train_df, val_df = train_test_split(df, test_size=0.2,random_state=42)

len(train_df), len(val_df)

"""###Filling/Removing Missing Values"""

print(train_df.isna().sum())
print(len(train_df))
print(val_df.isna().sum())
print(len(val_df))

#As the number of missing values is very less compared to the total size of the dataset, we drop the rows with null values
train_df = train_df.dropna()
val_df = val_df.dropna()

train_df.columns

input_cols = ['pickup_longitude', 'pickup_latitude',
       'dropoff_longitude', 'dropoff_latitude', 'passenger_count']

target_col = 'fare_amount'

"""###Training"""

train_inputs = train_df[input_cols]

train_targets = train_df[target_col]

"""####Validation"""

val_inputs = val_df[input_cols]

val_targets = val_df[target_col]

"""###Testing"""

test_inputs = test_df[input_cols]

"""##4. Training Hardcoded & Baseline Models

- It is necessary to have a base model which will provide us the minimum performance that we should get.                                       
- We obtain that minimum performance by simply predicting the fare amount for all trips as the mean of all the fare amounts in training set.
- This will help in evaluating the performance of the models that we will build later.
"""

class MeanRegressor:
  def fit(self, inputs, targets):
    self.mean = targets.mean()

  def predict(self, inputs):
    return np.full(inputs.shape[0],self.mean)

mean_model = MeanRegressor()

mean_model.fit(train_inputs, train_targets)

mean_model.mean

train_preds = mean_model.predict(train_inputs)

from sklearn.metrics import mean_squared_error

def rmse(targets,preds):
  return mean_squared_error(targets, preds, squared=False)

train_rmse = rmse(train_targets, train_preds)

train_rmse

val_preds = mean_model.predict(val_inputs)

val_rmse = rmse(val_targets, val_preds)

val_rmse

"""###Training a basic Linear Regression model"""

from sklearn.linear_model import LinearRegression
linear_model = LinearRegression()
linear_model.fit(train_inputs,train_targets)

train_preds = linear_model.predict(train_inputs)

train_preds

rmse(train_targets,train_preds)

val_preds = linear_model.predict(val_inputs)

val_preds

rmse(val_targets, val_preds)

"""##5. Feature Engineering
  - Add Date based features like hour, is_weekend, is_holiday
  - Add distance from popular landmarks
  - Add fuel prices feature
  - Add trip distance feature

###Adding Features related to date
  - month
  - is weekend
  - hour
  - is holiday
"""

import holidays

holidays_list = []
for item in holidays.USA(years=2016).items():
  holidays_list.append(item[0])

holidays_list

def add_dateparts(df, col):
  df['is_weekend'] = df[col].dt.day.isin([5,6])
  df['is_holiday'] = df[col].dt.date.isin(holidays_list)
  df['hour'] = df[col].dt.hour
  df['Date'] = df[col].dt.date

add_dateparts(train_df, 'pickup_datetime')
add_dateparts(val_df, 'pickup_datetime')
add_dateparts(test_df, 'pickup_datetime')

train_df.head()

"""###Calculate Journey Distance"""

import numpy as np

def haversine_np(lon1, lat1, lon2, lat2):
    """
    Calculate the great circle distance between two points
    on the earth (specified in decimal degrees)

    All args must be of equal length.

    """
    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])

    dlon = lon2 - lon1
    dlat = lat2 - lat1

    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2

    c = 2 * np.arcsin(np.sqrt(a))
    km = 6367 * c
    return km

def add_trip_distance(df):
    df['trip_distance'] = haversine_np(df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# add_trip_distance(train_df)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# add_trip_distance(val_df)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# add_trip_distance(test_df)

train_df.head()

"""###Calculate Distance from following landmarks
  - JFK Airport
  - LGA Airport
  - EWR Airport
  - Times Square
  - Met Meuseum
  - World Trade Center
"""

jfk_lonlat = -73.7781, 40.6413
lga_lonlat = -73.8740, 40.7769
ewr_lonlat = -74.1745, 40.6895
met_lonlat = -73.9632, 40.7794
wtc_lonlat = -74.0099, 40.7126

def add_landmark_dropoff_distance(df, landmark_name, landmark_lonlat):
    lon, lat = landmark_lonlat
    df[landmark_name + '_drop_distance'] = haversine_np(lon, lat, df['dropoff_longitude'], df['dropoff_latitude'])

# Commented out IPython magic to ensure Python compatibility.
# %%time
# for a_df in [train_df, val_df, test_df]:
#     for name, lonlat in [('jfk', jfk_lonlat), ('lga', lga_lonlat), ('ewr', ewr_lonlat), ('met', met_lonlat), ('wtc', wtc_lonlat)]:
#         add_landmark_dropoff_distance(a_df, name, lonlat)

train_df.sample(5)

"""###Add Fuel Prices"""

train_df.Date = train_df.Date.astype('datetime64[ns]')
val_df.Date = val_df.Date.astype('datetime64[ns]')
test_df.Date = test_df.Date.astype('datetime64[ns]')

len(train_df.Date.dt.date.unique())

x = df_fuel[df_fuel.Date.dt.date>train_df.Date.dt.date.min()]
x = x[x.Date.dt.date<train_df.Date.dt.date.max()]

len(x.Date.dt.date.unique())

df_fuel['Date'] = pd.to_datetime(df_fuel.Date, format='%Y-%m-%d')
df_fuel = df_fuel.set_index('Date').resample('D').ffill().reset_index()

x = df_fuel[df_fuel.Date.dt.date>train_df.Date.dt.date.min()]
x = x[x.Date.dt.date<train_df.Date.dt.date.max()]

len(x.Date.dt.date.unique())

df_fuel.Date = df_fuel.Date.astype('string')

df_fuel.Date.dtype

train_df.Date = train_df.Date.astype('string')
val_df.Date = val_df.Date.astype('string')
test_df.Date = test_df.Date.astype('string')

train_df = train_df.merge(df_fuel, how='left', on = ['Date'], validate='many_to_one') #merging the Fuel data into the train data

val_df = val_df.merge(df_fuel, how='left', on = ['Date'], validate='many_to_one') #merging the Fuel data into the validation data

test_df = test_df.merge(df_fuel, how='left', on = ['Date'], validate='many_to_one') # merging the Fuel data into the test data

"""###Impute Missing Values"""

train_df.isna().sum()

val_df.isna().sum()

test_df.isna().sum()

"""###Remove Outliers & Invalid Data"""

test_df.describe()

"""Taking into account the stats obtained above, we can use the following ranges:
  - fare_amount: $1 to  $500
  - longitudes: -75 to -72
  - latitudes: 40 to 42
  - passenger_count: 1 to 6
"""

def remove_outliers(df):
    return df[(df['fare_amount'] >= 1.) & (df['fare_amount'] <= 500.) & (df['pickup_longitude'] >= -75) & (df['pickup_longitude'] <= -72) &
              (df['dropoff_longitude'] >= -75) & (df['dropoff_longitude'] <= -72) & (df['pickup_latitude'] >= 40) & (df['pickup_latitude'] <= 42) &
              (df['dropoff_latitude'] >=40) & (df['dropoff_latitude'] <= 42) & (df['passenger_count'] >= 1) & (df['passenger_count'] <= 6)]

remove_outliers(train_df)

remove_outliers(val_df)

"""##6. Training & Evaluating Different Models
  - Ridge Regression
  - Gradient Boosting

####Preparing inputs & targets
"""

train_df.columns

input_cols = ['pickup_longitude', 'pickup_latitude',
       'dropoff_longitude', 'dropoff_latitude', 'passenger_count',
       'is_weekend', 'is_holiday', 'hour', 'trip_distance',
       'jfk_drop_distance', 'lga_drop_distance', 'ewr_drop_distance',
       'met_drop_distance', 'wtc_drop_distance',
       'New York State Average ($/gal)']

target_cols = 'fare_amount'

train_inputs = train_df[input_cols]
train_targets = train_df[target_cols]

train_inputs.head()

val_inputs =val_df[input_cols]
val_targets = val_df[target_cols]

test_inputs =test_df[input_cols]

"""Let's define a helper function to evaluate models"""

def evaluate(model, train_inputs, train_preds):
  print(train_inputs.head())
  train_preds = model.predict(train_inputs)
  train_rmse = mean_squared_error(train_targets, train_preds, squared=False)
  val_preds = model.predict(val_inputs)
  val_rmse = mean_squared_error(val_targets, val_preds, squared=False)
  return train_rmse, val_rmse, train_preds, val_preds

"""####Ridge Regression"""

from sklearn.linear_model import Ridge

ridge_model = Ridge(random_state=42, alpha=0.9)
ridge_model.fit(train_inputs, train_targets)
train_inputs.head()
evaluate(ridge_model, train_inputs, train_preds)

"""####Ridge Regression after standardization"""

train_inputs_list = list(train_inputs.columns)
train_inputs_list.remove('is_weekend')
train_inputs_list.remove('is_holiday')
train_inputs_list

X_train = train_inputs[train_inputs_list].values
y_train = train_targets.values.reshape((-1,1))
X_val = val_inputs[train_inputs_list].values
y_val = val_targets.values.reshape((-1,1))

y_train.shape, y_val.shape

X_train.shape

train_inputs.loc[:,['is_weekend','is_holiday']].values.shape

from sklearn.preprocessing import StandardScaler
sc_x = StandardScaler()
sc_y = StandardScaler()
X_train = sc_x.fit_transform(X_train)
y_train = sc_y.fit_transform(y_train)
X_train = np.append(X_train, train_inputs.loc[:,['is_weekend','is_holiday']].values,axis=1)
X_val = sc_x.transform(X_val)
X_val = np.append(X_val, val_inputs.loc[:,['is_weekend','is_holiday']].values,axis=1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = Ridge(random_state=42, alpha=0.9)
# model.fit(X_train, y_train)

y_preds_train = sc_y.inverse_transform(model.predict(X_train).reshape(-1,1))
y_preds_val = sc_y.inverse_transform(model.predict(X_val).reshape(-1,1))

train_s_rmse = mean_squared_error(y_train, y_preds_train, squared=False)
val_s_rmse = mean_squared_error(y_val, y_preds_val, squared=False)

train_s_rmse, val_s_rmse

"""####XGBoost Regression"""

from xgboost import XGBRegressor
regressor = XGBRegressor(max_depth=5, random_state=42)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# regressor.fit(train_inputs, train_targets)

train_m_preds = regressor.predict(train_inputs)
train_m_rmse = mean_squared_error(train_targets, train_m_preds, squared=False)
val_m_preds = regressor.predict(val_inputs)
val_m_rmse = mean_squared_error(val_targets, val_m_preds, squared=False)

train_m_rmse

val_m_rmse

"""##7. Predicting Fares for test data & Creating the submission file"""

test_preds = regressor.predict(test_inputs)

submission = pd.read_csv(
    data_dir+'/test.csv',)

submission['fare_amount'] = test_preds

submission = submission.loc[:,['key','fare_amount']]

submission

# @title fare_amount

from matplotlib import pyplot as plt
submission['fare_amount'].plot(kind='hist', bins=20, title='fare_amount')
plt.gca().spines[['top', 'right',]].set_visible(False)

submission.isna().sum()

submission.to_csv('submission.csv',index=False)